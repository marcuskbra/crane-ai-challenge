# Makefile for crane-challenge
# Modern Python project with Clean Architecture

.PHONY: help install dev-install setup first-run test lint lint-fix format format-fix type-check coverage validate clean run stop-all \
	test-all test-unit test-integration test-fast api-dev api-prod api-test api-docs api-health \
	backend-dev backend-stop backend-logs \
	ui-install ui-dev ui-build ui-clean ui-lint ui-test \
	llm-local-setup llm-local-pull llm-local-pull-fast llm-local-start llm-local-stop llm-local-test \
	llm-check llm-status llm-models llm-config-check

# ============================================================================
# Help & Documentation
# ============================================================================

help: ## Show this help message
	@echo "crane-challenge - Available commands:"
	@echo ""
	@echo "Setup & Installation:"
	@grep -E '^(setup|first-run|install|dev-install):.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-25s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Development:"
	@grep -E '^(run|test|lint|format|type-check|validate):.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-25s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "API Development:"
	@grep -E '^(api-.*|backend-.*):.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-25s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Frontend UI (Visualization):"
	@grep -E '^(ui-.*):.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-25s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Testing:"
	@grep -E '^(test[^-]|test-all|test-integration|test-fast|coverage):.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-25s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Local LLM Testing:"
	@grep -E '^(llm-.*):.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-25s\033[0m %s\n", $$1, $$2}'
	@echo ""
	@echo "Cleanup:"
	@grep -E '^(clean.*|stop-all):.*?## .*$$' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = ":.*?## "}; {printf "  \033[36m%-25s\033[0m %s\n", $$1, $$2}'

# ============================================================================
# Setup & Installation
# ============================================================================

install: ## Install production dependencies
	uv sync --no-dev

dev-install: ## Install all dependencies including dev and test extras
	uv sync --all-extras
	@echo "Installing tox globally for convenience..."
	uv tool install tox --with tox-uv
	@echo "Installing pre-commit hooks..."
	pre-commit install
	@echo ""
	@echo "âœ… Dependencies installed!"
	@echo ""
	@echo "ðŸ“‹ Next steps:"
	@echo "  1. Copy .env.example to .env and configure LLM credentials"
	@echo "  2. Run: make llm-config-check (to verify configuration)"
	@echo "  3. Run: make test (to verify everything works)"

setup: first-run ## Alias for first-run (complete automated setup)

first-run: ## ðŸš€ Complete first-time setup (deps + native LLM + backend + optional frontend)
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘          ðŸš€ Crane Challenge - First-Time Setup                 â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "This will:"
	@echo "  1. Check prerequisites (Docker, uv)"
	@echo "  2. Install all dependencies"
	@echo "  3. Setup .env configuration"
	@echo "  4. Start native LLM services (Ollama + LiteLLM)"
	@echo "  5. Start backend API in background"
	@echo "  6. Optionally start frontend UI"
	@echo ""
	@read -p "Continue? [y/N] " response; \
	if [ "$$response" != "y" ] && [ "$$response" != "Y" ]; then \
		echo "Setup cancelled."; \
		exit 1; \
	fi
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Step 1/5: Checking Prerequisites"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@if ! command -v docker >/dev/null 2>&1; then \
		echo "âŒ Docker not found. Please install Docker Desktop:"; \
		echo "   https://www.docker.com/products/docker-desktop"; \
		exit 1; \
	fi
	@echo "âœ… Docker installed: $$(docker --version)"
	@if ! command -v uv >/dev/null 2>&1; then \
		echo "âŒ uv not found. Installing..."; \
		curl -LsSf https://astral.sh/uv/install.sh | sh; \
	fi
	@echo "âœ… uv installed: $$(uv --version)"
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Step 2/5: Installing Dependencies"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@make dev-install
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Step 3/5: Setting up .env Configuration"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@if [ ! -f .env ]; then \
		echo "ðŸ“ Creating .env from .env.example..."; \
		cp .env.example .env; \
		echo "âœ… .env file created"; \
		echo ""; \
		echo "âš ï¸  Using default configuration (local LLM via Docker)"; \
		echo "   To use cloud providers (OpenAI/Anthropic), edit .env"; \
	else \
		echo "âœ… .env file already exists (keeping your configuration)"; \
	fi
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Step 4/5: Setting Up Native Local LLM Services"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "ðŸš€ Installing and configuring Ollama + LiteLLM..."
	@make llm-local-setup
	@echo ""
	@echo "ðŸ“¥ Pulling qwen2.5:3b model (~1.9GB)..."
	@ollama pull qwen2.5:3b
	@echo "âœ… Model downloaded!"
	@echo ""
	@echo "ðŸš€ Starting LiteLLM proxy..."
	@make llm-local-start
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Step 5/5: Verifying Configuration"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@make llm-config-check
	@echo ""
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘              âœ… Setup Complete - Ready to Use!                 â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Starting Backend API"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@make backend-dev
	@echo ""
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo "Optional: Start Frontend UI"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@read -p "ðŸŽ¨ Start frontend UI? [y/N] " start_frontend; \
	echo ""; \
	if [ "$$start_frontend" = "y" ] || [ "$$start_frontend" = "Y" ]; then \
		echo "ðŸŽ¨ Starting Frontend UI in background..."; \
		echo "   â†’ Running on: http://localhost:3000"; \
		echo "   â†’ Will open in browser automatically"; \
		echo ""; \
		nohup make ui-dev > /tmp/crane-ui.log 2>&1 & \
		sleep 3; \
		echo "âœ… Frontend started in background (PID: $$!)"; \
		echo ""; \
		echo "ðŸ“ Frontend Management:"; \
		echo "  â€¢ View logs:     tail -f /tmp/crane-ui.log"; \
		echo "  â€¢ Stop frontend: make stop-all"; \
		echo "  â€¢ Or kill process on port 3000"; \
		echo ""; \
	fi; \
	echo ""; \
	echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘              ðŸŽ‰ All Services Running!                          â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "ðŸ”— Service URLs:"; \
	echo "  â€¢ Backend API:         http://localhost:8000"; \
	echo "  â€¢ API Docs:            http://localhost:8000/api/docs"; \
	echo "  â€¢ Frontend UI:         http://localhost:3000"; \
	echo ""; \
	echo "ðŸŽ¯ Quick Commands:"; \
	echo "  â€¢ Test everything:     make test-all"; \
	echo "  â€¢ Backend logs:        make backend-logs"; \
	echo "  â€¢ Frontend logs:       tail -f /tmp/crane-ui.log"; \
	echo "  â€¢ Stop all services:   make stop-all"; \
	echo "  â€¢ Check LLM status:    make llm-status"; \
	echo ""; \
	echo "ðŸ¤– Native LLM Services:"; \
	echo "  â€¢ View logs:           tail -f /tmp/litellm.log"; \
	echo "  â€¢ Stop services:       make llm-local-stop"; \
	echo "  â€¢ Restart services:    make llm-local-stop && make llm-local-start"; \
	echo "  â€¢ Pull more models:    ollama pull qwen2.5:3b"; \
	echo ""; \
	echo "ðŸ“š Documentation:"; \
	echo "  â€¢ README.md"; \
	echo "  â€¢ docs/architecture.md"; \
	echo "  â€¢ docs/multi_provider_llm.md"; \
	echo ""

# ============================================================================
# Development & Running
# ============================================================================

run: ## Run the application
	uv run python -m challenge

# ============================================================================
# API Development
# ============================================================================

api-dev: ## Run the API server in development mode (auto-reload)
	uv run uvicorn challenge.presentation.main:app --reload --host 0.0.0.0 --port 8000

api-prod: ## Run the API server in production mode
	uv run uvicorn challenge.presentation.main:app --host 0.0.0.0 --port 8000 --workers 4

api-test: ## Run API tests only
	uv run pytest tests/unit/presentation/api/ -xvs

api-health: ## Check API health endpoint
	@echo "Checking API health..."
	@curl -s http://localhost:8000/api/v1/health | python -m json.tool || echo "API is not running. Start with 'make api-dev'"

api-docs: ## Open API documentation in browser
	@echo "Opening API docs at http://localhost:8000/api/docs"
	@python -m webbrowser http://localhost:8000/api/docs || open http://localhost:8000/api/docs || xdg-open http://localhost:8000/api/docs

backend-dev: ## Run backend API in background (similar to ui-dev)
	@echo "ðŸš€ Starting backend API server in background..."
	@if lsof -ti:8000 >/dev/null 2>&1; then \
		echo "âš ï¸  Backend already running on port 8000"; \
		echo "   Stop it with: make backend-stop"; \
	else \
		nohup uv run python -m challenge > /tmp/crane-backend.log 2>&1 & \
		echo $$! > .backend.pid; \
		sleep 2; \
		if lsof -ti:8000 >/dev/null 2>&1; then \
			echo "âœ… Backend started successfully (PID: $$(cat .backend.pid))"; \
			echo "   API: http://localhost:8000"; \
			echo "   Docs: http://localhost:8000/api/docs"; \
			echo "   Logs: tail -f /tmp/crane-backend.log"; \
			echo "   Stop: make backend-stop"; \
		else \
			echo "âŒ Backend failed to start"; \
			echo "   Check logs: cat /tmp/crane-backend.log"; \
			rm -f .backend.pid; \
		fi; \
	fi

backend-stop: ## Stop backend API server
	@echo "ðŸ›‘ Stopping backend API server..."
	@if [ -f .backend.pid ]; then \
		PID=$$(cat .backend.pid); \
		if ps -p $$PID > /dev/null 2>&1; then \
			kill $$PID 2>/dev/null && echo "   âœ… Backend stopped (PID: $$PID)" || echo "   âš ï¸  Could not stop backend"; \
		else \
			echo "   â„¹ï¸  Backend process not found (cleaning up stale PID file)"; \
		fi; \
		rm -f .backend.pid; \
	elif lsof -ti:8000 >/dev/null 2>&1; then \
		kill -9 $$(lsof -ti:8000) 2>/dev/null && echo "   âœ… Backend stopped (port 8000)" || echo "   âš ï¸  Could not stop backend"; \
	else \
		echo "   â„¹ï¸  No backend running on port 8000"; \
	fi

backend-logs: ## View backend API logs
	@echo "ðŸ“‹ Backend API logs (Ctrl+C to exit):"
	@tail -f /tmp/crane-backend.log

# ============================================================================
# Frontend UI (Visualization Tool)
# ============================================================================

ui-install: ## Install frontend dependencies
	@echo "ðŸ“¦ Installing frontend dependencies..."
	cd ui-react && npm install
	@echo "âœ… Frontend dependencies installed!"

ui-dev: ## Run frontend development server
	@echo "ðŸš€ Starting frontend development server..."
	@echo "ðŸ“ Note: This is a visualization tool, not production-ready"
	@echo "ðŸŒ Frontend will be available at http://localhost:3000"
	cd ui-react && npm run dev

ui-build: ## Build frontend for production (visualization only)
	@echo "ðŸ—ï¸  Building frontend..."
	cd ui-react && npm run build
	@echo "âœ… Frontend build complete (dist/)"

ui-clean: ## Clean frontend build artifacts and dependencies
	@echo "ðŸ§¹ Cleaning frontend files..."
	cd ui-react && rm -rf node_modules dist .vite
	@echo "âœ… Frontend cleanup complete!"

ui-lint: ## Run frontend linting
	@echo "ðŸ” Linting frontend code..."
	cd ui-react && npm run lint || echo "âš ï¸  Linting issues found"

ui-test: ## Run frontend tests (if available)
	@echo "ðŸ§ª Running frontend tests..."
	@echo "âš ï¸  Frontend tests not implemented (visualization tool only)"

# ============================================================================
# Testing
# ============================================================================

test: ## Run unit tests (default)
	uv run pytest tests/unit/ -xvs --tb=short

test-all: ## Run all tests (unit + integration)
	uv run pytest tests/ -xvs --tb=short

test-unit: ## Run unit tests explicitly
	uv run pytest tests/unit/ -xvs

test-integration: ## Run integration tests only
	uv run pytest tests/integration/ -xvs --tb=short

test-fast: ## Run tests quickly (less verbose)
	uv run pytest tests/unit/ -x -q

coverage: ## Run tests with coverage report
	uv run pytest tests/ \
		--cov=src/challenge \
		--cov-report=term-missing \
		--cov-report=html \
		--cov-report=xml
	@echo "Coverage report generated in htmlcov/index.html"

# ============================================================================
# Local LLM Testing
# ============================================================================

stop-all: ## ðŸ›‘ Stop all services (native LLM + backend + frontend)
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘              ðŸ›‘ Stopping All Services...                       â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "1ï¸âƒ£  Stopping native LLM services (LiteLLM + Ollama)..."
	@make llm-local-stop
	@echo ""
	@echo "2ï¸âƒ£  Stopping backend API..."
	@make backend-stop
	@echo ""
	@echo "3ï¸âƒ£  Stopping frontend UI (port 3000)..."
	@if lsof -ti:3000 >/dev/null 2>&1; then \
		kill -9 $$(lsof -ti:3000) 2>/dev/null && echo "   âœ… Frontend stopped (port 3000)" || echo "   âš ï¸  Could not stop frontend"; \
	else \
		echo "   â„¹ï¸  No frontend running on port 3000"; \
	fi
	@echo ""
	@echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
	@echo "â•‘              âœ… All Services Stopped!                          â•‘"
	@echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "ðŸ“ Note: Redis/Postgres Docker containers kept running."
	@echo "   Stop them with: docker compose down"
	@echo ""

# Local Development LLM Testing
# ------------------------------

llm-local-setup: ## Install Ollama and LiteLLM for local development
	@echo "ðŸ“¦ Installing local LLM dependencies..."
	@echo ""
	@echo "1ï¸âƒ£  Checking Ollama installation..."
	@if command -v ollama >/dev/null 2>&1; then \
		echo "   âœ… Ollama already installed ($$(ollama --version))"; \
	else \
		echo "   ðŸ“¥ Installing Ollama..."; \
		if [[ "$$(uname)" == "Darwin" ]]; then \
			if command -v brew >/dev/null 2>&1; then \
				brew install ollama; \
			else \
				echo "   âš ï¸  Homebrew not found. Install from https://ollama.ai"; \
				exit 1; \
			fi; \
		else \
			curl -fsSL https://ollama.ai/install.sh | sh; \
		fi; \
	fi
	@echo ""
	@echo "2ï¸âƒ£  Installing LiteLLM..."
	@if command -v litellm >/dev/null 2>&1; then \
		echo "   âœ… LiteLLM already installed ($$(litellm --version 2>/dev/null || echo 'version unknown'))"; \
	else \
		uv tool install 'litellm[proxy]'; \
		echo "   âœ… LiteLLM installed with proxy extras"; \
	fi
	@echo ""
	@echo "3ï¸âƒ£  Starting Ollama service..."
	@if [[ "$$(uname)" == "Darwin" ]]; then \
		brew services start ollama 2>/dev/null || ollama serve & \
	else \
		systemctl start ollama 2>/dev/null || ollama serve & \
	fi
	@sleep 5
	@echo "   âœ… Ollama service started"
	@echo ""
	@echo "âœ… Local LLM setup complete!"
	@echo "ðŸ“ Next: make llm-local-pull (to download models)"

llm-local-pull: ## Pull recommended LLM models
	@echo "ðŸ“¥ Pulling recommended models..."
	@echo ""
	@echo "1ï¸âƒ£  Pulling Qwen2.5-3B (best quality, 2.3GB)..."
	ollama pull qwen2.5:3b
	@echo ""
	@echo "2ï¸âƒ£  Pulling Qwen2.5-1.5B (faster, 1GB)..."
	ollama pull qwen2.5:1.5b
	@echo ""
	@echo "3ï¸âƒ£  Pulling Phi-3-mini (alternative, 2.2GB)..."
	ollama pull phi3:mini
	@echo ""
	@echo "âœ… Models downloaded!"
	@echo "ðŸ“ Next: make llm-local-start (to start proxy)"

llm-local-pull-fast: ## Pull only fast/small models (1.5B)
	@echo "ðŸ“¥ Pulling fast models for straightforward prompts..."
	@echo ""
	@echo "1ï¸âƒ£  Pulling Qwen2.5-1.5B (fast, 1GB)..."
	ollama pull qwen2.5:1.5b
	@echo ""
	@echo "âœ… Fast model downloaded!"
	@echo "ðŸ“ Use: export OPENAI_MODEL=qwen2.5:1.5b"
	@echo "ðŸ“ Then: make llm-local-start"

llm-local-start: ## Start LiteLLM proxy for local development
	@./scripts/start-litellm.sh

llm-local-stop: ## Stop LiteLLM proxy and Ollama service
	@./scripts/stop-litellm.sh
	@echo "ðŸ›‘ Stopping Ollama service..."
	@if [[ "$$(uname)" == "Darwin" ]]; then \
		brew services stop ollama 2>/dev/null || pkill -f "ollama serve" 2>/dev/null || true; \
	else \
		systemctl stop ollama 2>/dev/null || pkill -f "ollama serve" 2>/dev/null || true; \
	fi
	@echo "âœ… All LLM services stopped!"

llm-local-test: ## Run tests using local LLM (proxy must be running)
	@echo "ðŸ§ª Running tests with local LLM..."
	@echo "ðŸ“ Ensure LiteLLM proxy is running: make llm-local-start"
	@echo ""
	OPENAI_BASE_URL=http://localhost:4000 \
	OPENAI_MODEL=qwen2.5:3b \
	uv run pytest tests/ -xvs --tb=short -m "not openai"
	@echo ""
	@echo "âœ… Tests completed with local LLM!"

# LLM Utilities
# --------------

llm-config-check: ## Verify LLM configuration (API keys, base URL, etc.)
	@echo "ðŸ” Verifying LLM configuration..."
	@uv run python scripts/verify_llm_config.py

llm-check: ## Check local LLM installation status
	@echo "ðŸ” Checking local LLM installation..."
	@echo ""
	@echo "Ollama:"
	@if command -v ollama >/dev/null 2>&1; then \
		echo "  âœ… Installed: $$(ollama --version)"; \
		if pgrep -f "ollama serve" >/dev/null 2>&1; then \
			echo "  âœ… Service: Running"; \
		else \
			echo "  âš ï¸  Service: Not running"; \
		fi; \
	else \
		echo "  âŒ Not installed"; \
	fi
	@echo ""
	@echo "LiteLLM:"
	@if command -v litellm >/dev/null 2>&1; then \
		echo "  âœ… Installed: $$(litellm --version 2>/dev/null || echo 'version unknown')"; \
		if pgrep -f "litellm" >/dev/null 2>&1; then \
			echo "  âœ… Proxy: Running at http://localhost:4000"; \
		else \
			echo "  âš ï¸  Proxy: Not running"; \
		fi; \
	else \
		echo "  âŒ Not installed"; \
	fi
	@echo ""
	@echo "Models:"
	@if command -v ollama >/dev/null 2>&1; then \
		ollama list 2>/dev/null || echo "  âš ï¸  Unable to list models (is Ollama running?)"; \
	else \
		echo "  âš ï¸  Ollama not installed"; \
	fi

llm-status: ## Show status of native LLM services and configuration
	@echo "ðŸ“Š Native LLM Status"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@make llm-check
	@echo ""
	@echo "Configuration:"
	@echo "  Base URL: $${OPENAI_BASE_URL:-http://localhost:4000}"
	@echo "  Model: $${OPENAI_MODEL:-qwen2.5:3b}"
	@echo "  Config: config/litellm_config.yaml"
	@echo ""
	@echo "Quick Commands:"
	@echo "  Start:  make llm-local-start"
	@echo "  Test:   make llm-local-test"
	@echo "  Stop:   make llm-local-stop"
	@echo "  Logs:   tail -f /tmp/litellm.log"

llm-models: ## List available and downloaded LLM models
	@echo "ðŸ“¦ Available LLM Models"
	@echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
	@echo ""
	@echo "Downloaded models:"
	@if command -v ollama >/dev/null 2>&1; then \
		ollama list 2>/dev/null || echo "âš ï¸  Unable to list models (is Ollama running?)"; \
	else \
		echo "âŒ Ollama not installed"; \
	fi
	@echo ""
	@echo "Recommended models:"
	@echo "  âš¡ qwen2.5:1.5b - FASTEST (1GB, 2-3x faster, 91% accuracy)"
	@echo "  ðŸŽ¯ qwen2.5:3b  - Best quality (2.3GB, 97% accuracy)"
	@echo "  ðŸ”„ phi3:mini   - Alternative (2.2GB, 96% accuracy)"
	@echo ""
	@echo "For straightforward prompts: make llm-local-pull-fast"
	@echo "For all models: make llm-local-pull"

# ============================================================================
# Code Quality
# ============================================================================

lint: ## Run linter (ruff)
	uv run ruff check src/ tests/

lint-fix: ## Run linter and auto-fix issues
	uv run ruff check src/ tests/ --fix

format: ## Format code with ruff
	uv run ruff format src/ tests/

format-check: ## Check if code is properly formatted
	uv run ruff format src/ tests/ --check

type-check: ## Run type checking with ty
	uv run ty check src/ tests/

validate: ## Run all validation steps (tests, lint, format, type-check)
	@echo "ðŸ§ª Running tests..."
	@make test-fast
	@echo ""
	@echo "ðŸ” Running linter..."
	@make lint
	@echo ""
	@echo "ðŸ“ Checking format..."
	@make format-check
	@echo ""
	@echo "ðŸ”Ž Running type checker..."
	@make type-check
	@echo ""
	@echo "âœ… All validation checks passed!"

# ============================================================================
# Cleanup
# ============================================================================

clean: ## Clean Python cache and build files
	@echo "ðŸ§¹ Cleaning Python files..."
	@find . -type d -name "__pycache__" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name ".pytest_cache" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name ".ruff_cache" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name "htmlcov" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name "*.egg-info" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name "build" -exec rm -rf {} + 2>/dev/null || true
	@find . -type d -name "dist" -exec rm -rf {} + 2>/dev/null || true
	@find . -type f -name "*.pyc" -delete 2>/dev/null || true
	@find . -type f -name ".coverage" -delete 2>/dev/null || true
	@find . -type f -name "coverage.xml" -delete 2>/dev/null || true
	@find . -type f -name "*.py.bak" -delete 2>/dev/null || true
	@rm -rf .tox 2>/dev/null || true
	@echo "âœ… Python cleanup complete!"

# ============================================================================
# Tox Commands (for CI/CD and multi-environment testing)
# ============================================================================

tox-unit: ## Run unit tests via tox
	tox -e unit

tox-integration: ## Run integration tests via tox
	tox -e integration

tox-coverage: ## Run coverage via tox
	tox -e coverage

tox-validate: ## Run all validation via tox
	tox -e validate

tox-py312: ## Run tests on Python 3.12 specifically
	tox -e py312

# ============================================================================
# Utility Commands
# ============================================================================

deps-tree: ## Show dependency tree
	uv tree

deps-outdated: ## Show outdated dependencies
	uv tree --outdated

deps-upgrade: ## Upgrade all dependencies and update lock file
	uv lock --upgrade
	uv sync --all-extras

version: ## Show project version
	@python -c "import tomllib; print(tomllib.load(open('pyproject.toml', 'rb'))['project']['version'])"

# ============================================================================
# Development Shortcuts
# ============================================================================

fix: lint-fix format ## Auto-fix all code issues (lint + format)

check: lint format-check type-check ## Run all checks without tests

quick: test-fast lint format-check ## Quick validation (fast tests + quality checks)
