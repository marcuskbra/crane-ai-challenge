# Crane AI Agent Runtime - Environment Configuration
# Copy this file to .env and fill in your values

# ⚠️  IMPORTANT: LLM configuration is REQUIRED
# ============================================
# The application will fail loudly if credentials are missing or invalid.
# Pattern-based fallback is ONLY used for transient errors (rate limits, outages).
# After configuring, verify with: make llm-config-check

# ==============================================================================
# LLM Configuration (LiteLLM Multi-Provider Support)
# ==============================================================================

# LLM Provider Selection
# ----------------------
# Default: Local LLM via LiteLLM proxy (free, fast, private)
# Alternatives: Switch to cloud providers by editing configuration below
LLM_PROVIDER=openai

# LLM Model Selection
# -------------------
# Default: Local model via LiteLLM proxy
# Local models (Ollama): qwen2.5:3b (default), llama3.2:3b, phi3:3b
# Cloud models - OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo
# Cloud models - Anthropic: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022
LLM_MODEL=qwen2.5:3b

# LLM Temperature (0.0-2.0)
# -------------------------
# 0.0 = Deterministic (recommended for planning)
# 0.1 = Consistent (default)
# 1.0 = Balanced
# 2.0 = Creative
LLM_TEMPERATURE=0.1

# LLM Base URL
# -------------
# Default: Local LiteLLM proxy (configured by make setup)
# For cloud providers: Leave empty or comment out
LLM_BASE_URL=http://localhost:4000

# LLM API Key
# -----------
# Default: Dummy key for local LLM (no real API key needed)
# For cloud providers: Replace with actual API key from provider
LLM_API_KEY=sk-local-llm-dummy-key

# ==============================================================================
# Cloud Provider Configurations (Optional)
# ==============================================================================
# Uncomment and configure ONE of the options below to use cloud providers
# instead of local LLM. Remember to also update LLM_MODEL above.

# Option 1: OpenAI (Cloud Provider)
# ----------------------------------
# Get your API key from: https://platform.openai.com/api-keys
# LLM_PROVIDER=openai
# LLM_MODEL=gpt-4o-mini
# LLM_API_KEY=sk-your-openai-api-key-here
# LLM_BASE_URL=  # Leave empty for cloud
# OR use legacy variable names:
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL=gpt-4o-mini

# Option 2: Anthropic Claude (Alternative Cloud Provider)
# --------------------------------------------------------
# Get your API key from: https://console.anthropic.com/
# Uncomment to use Claude instead of OpenAI:
# LLM_PROVIDER=anthropic
# LLM_API_KEY=sk-ant-your-anthropic-api-key-here
# LLM_MODEL=claude-3-5-sonnet-20241022  # or claude-3-5-haiku-20241022

# Option 3: Local LLM via Ollama (Testing/Development - Free)
# ------------------------------------------------------------
# Requires: Ollama running locally
# Setup: https://ollama.ai/download
# Uncomment these to use local LLM:
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:11434/v1
# LLM_MODEL=qwen2.5:3b  # or llama3.2:3b, phi3:3b
# LLM_API_KEY is automatically set to dummy value when LLM_BASE_URL is set

# Option 4: Local LLM via LiteLLM Proxy (Advanced)
# -------------------------------------------------
# Requires: LiteLLM proxy running (see claudedocs/local_llm_testing_guide.md)
# LLM_PROVIDER=ollama
# LLM_BASE_URL=http://localhost:4000
# LLM_MODEL=qwen2.5:3b
# OR use legacy variable names:
# OPENAI_BASE_URL=http://localhost:4000
# OPENAI_MODEL=qwen2.5:3b

# ==============================================================================
# Application Configuration
# ==============================================================================

# Environment (development, staging, production)
ENVIRONMENT=development

# API Server Configuration
HOST=0.0.0.0
PORT=8000

# Debug Mode (enables auto-reload and detailed error messages)
DEBUG=true

# Log Level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ==============================================================================
# Orchestrator Configuration
# ==============================================================================

# Maximum retry attempts for failed steps
MAX_RETRIES=3

# Timeout for each step execution (seconds)
STEP_TIMEOUT=30.0

# ==============================================================================
# CORS Configuration (for frontend integration)
# ==============================================================================

# Comma-separated list of allowed origins
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# ==============================================================================
# Security (Optional - for production)
# ==============================================================================

# API Key for authentication (if implementing auth)
# API_KEY=your-secret-api-key

# ==============================================================================
# Notes
# ==============================================================================

# 1. NEVER commit .env to version control (already in .gitignore)
# 2. Keep API keys secure and rotate them regularly
# 3. Use different .env files for different environments
# 4. LLM configuration is REQUIRED - missing/invalid credentials will cause failure
# 5. Pattern-based fallback is ONLY for transient errors (rate limits, service outages)
# 6. Verify configuration with: make llm-config-check
# 7. For production, set DEBUG=false and use appropriate LOG_LEVEL
# 8. LiteLLM provides unified interface for OpenAI, Anthropic, and local models
# 9. Legacy OPENAI_* variable names still work for backward compatibility
# 10. Local LLMs (Ollama) are free and great for development/testing
# 11. Cost tracking automatically adjusts for different providers via LiteLLM
